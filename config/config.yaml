name: "doma"

model:
  sample_rate: 16000
  log_prediction: true 
  skip_nan_grad: false
  ckpt_dir: null

  train_ds:
    manifest_filepath: null
    transcript_filepath: null #<----- 用于特征融合
    sample_rate: ${model.sample_rate}
    batch_size: 16
    shuffle: true
    num_workers: 8
    pin_memory: true
    use_start_end_token: true
    trim_silence: false
    max_duration: 11.0
    min_duration: 0.0
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    bucketing_strategy: "synced_randomized"
    bucketing_batch_size: null

  validation_ds:
    manifest_filepath: null
    transcript_filepath: null 
    sample_rate: ${model.sample_rate}
    batch_size: 16
    shuffle: false
    num_workers: 8
    pin_memory: true
    use_start_end_token: true

  test_ds:
    manifest_filepath: null
    transcript_filepath: null 
    sample_rate: ${model.sample_rate}
    batch_size: 16
    shuffle: false
    num_workers: 8
    pin_memory: true
    use_start_end_token: true

  tokenizer:
    dir: ???  
    type: bpe

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    sample_rate: ${model.sample_rate}
    normalize: "per_feature"
    window_size: 0.025
    window_stride: 0.01
    window: "hann"
    features: 80
    n_fft: 512
    log: true
    frame_splicing: 1
    dither: 0.00001
    pad_to: 0
    pad_value: 0.0

  spec_augment:
    _target_: nemo.collections.asr.modules.SpectrogramAugmentation
    freq_masks: 2 
    time_masks: 10 
    freq_width: 27
    time_width: 0.05

  enc_dim: 768

  llada_gate:
    freeze: False

  encoder:
    freeze: true

  embedding:
    _target_: nemo.collections.asr.modules.transformer.TransformerEmbedding
    vocab_size: -1
    hidden_size: ${model.enc_dim}
    max_sequence_length: 512
    num_token_types: 1
    embedding_dropout: 0.0
    learn_positional_encodings: false

  decoder:
    _target_: nemo.collections.asr.modules.transformer.TransformerDecoder
    num_layers: 3 
    hidden_size: ${model.enc_dim}
    inner_size: 2048
    num_attention_heads: 8
    attn_score_dropout: 0.0
    attn_layer_dropout: 0.0
    ffn_dropout: 0.0

  classifier:
    _target_: nemo.collections.asr.parts.submodules.token_classifier.TokenClassifier
    hidden_size: ${model.enc_dim}
    num_classes: -1
    num_layers: 1
    activation: 'relu'
    log_softmax: true

  loss:
    label_smoothing: 0.0

  sequence_generator:
    type: greedy
    max_sequence_length: ${model.embedding.max_sequence_length}
    temperature: 1.25 
    beam_size: 32  
    len_pen: 0 

  optim_param_groups: 
      llada_gate:
        lr: 0.00001

trainer:
  devices: 1 
  num_nodes: 1
  max_epochs: 10
  max_steps: -1 
  val_check_interval: 1.0 
  accelerator: auto
  strategy: ddp_find_unused_parameters_true
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  precision: 32 
  log_every_n_steps: 20  
  num_sanity_val_steps: 0 
  check_val_every_n_epoch: 1 
  sync_batchnorm: true
  enable_checkpointing: False  
  logger: false  
  benchmark: false 

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  checkpoint_callback_params: 
    monitor: "val_wer"
    mode: "min"
    save_top_k: 5
    always_save_nemo: true 
    save_best_model: false

  resume_if_exists: false
  resume_ignore_no_checkpoint: false
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null

  
